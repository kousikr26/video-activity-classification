{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video_action_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETsFNnK4RQS6",
        "colab_type": "text"
      },
      "source": [
        "# Video Activity classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS7V8AEIZHMp",
        "colab_type": "text"
      },
      "source": [
        "# Task: Classify the actions in a breakfast preparation video dataset using sequence models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUPOPxi7ZZfF",
        "colab_type": "text"
      },
      "source": [
        "Datasaet is present in my google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_G3CVG8LqJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6dbfbea9-65bc-41a5-c6f5-d1c560fbfb9d"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bNOYenpgTz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4cb32cdd-1d50-4e20-990d-c90fb9492ea8"
      },
      "source": [
        "path=\"/content/gdrive/My Drive/video_action_classification/nndataex\"\n",
        "!ls '/content/gdrive/My Drive/video_action_classification/nndataex'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "breakfast_i3d.7z  groundTruth\t  read_datasetBreakfast.py\n",
            "data\t\t  groundTruth.7z  splits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quxfFstfZqM5",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary libraries for data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9-gx_CO2cHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "13f84d88-f2aa-4975-d5ad-b7a00cd7b956"
      },
      "source": [
        "import os  \n",
        "import torch\n",
        "import numpy as np\n",
        "import os.path \n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afw4YmY_bCx3",
        "colab_type": "text"
      },
      "source": [
        "I used keras to implement the model\n",
        "\n",
        "Importing necessary keras layers and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQq81Mbgjzqa",
        "colab_type": "code",
        "outputId": "c39b49f9-b4fd-4d4b-eaea-bcc1c5a98b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_cwVUASZwQV",
        "colab_type": "text"
      },
      "source": [
        "Code given in the readBreakfast.py file which loads the data into torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuH5qq6caqeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _isArrayLike(obj):\n",
        "    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\n",
        "\n",
        " \n",
        "def load_data(split_load, actions_dict, GT_folder, DATA_folder):\n",
        "    file_ptr = open(split_load, 'r')\n",
        "    content_all = file_ptr.read().split('\\n')[1:-1]\n",
        "    content_all = [x.strip('./data/groundTruth/') + 't' for x in content_all]\n",
        "  \n",
        "    all_tasks = ['tea', 'cereals', 'coffee', 'friedegg', 'juice', 'milk', 'sandwich', 'scrambledegg', 'pancake', 'salat']\n",
        "\n",
        "    data_breakfast = []\n",
        "    labels_breakfast = []\n",
        "    tasks_breakfast = []\n",
        "    for i in tqdm(range(len(content_all))):\n",
        "        content=content_all[i]\n",
        "        curr_task = content.split('_')[-1].split('.')[0]\n",
        "        tasks_breakfast.append(int( all_tasks.index(curr_task)) )\n",
        "\n",
        "        file_ptr = open( GT_folder + content, 'r')\n",
        "        curr_gt = file_ptr.read().split('\\n')[:-1]\n",
        "        label_seq, length_seq = get_label_length_seq(curr_gt)\n",
        "\n",
        "        loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n",
        "        curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n",
        "\n",
        "        label_curr_video = []\n",
        "        for iik in range(len(curr_gt)):\n",
        "            label_curr_video.append( actions_dict[curr_gt[iik]] )\n",
        "  \n",
        "        data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n",
        "        labels_breakfast.append(label_curr_video )\n",
        " \n",
        "    return   data_breakfast, labels_breakfast, tasks_breakfast\n",
        "\n",
        "\n",
        "def get_label_bounds( data_labels):\n",
        "    labels_uniq = []\n",
        "    labels_uniq_loc = []\n",
        "    for kki in range(0, len(data_labels) ):\n",
        "        uniq_group, indc_group = get_label_length_seq(data_labels[kki])\n",
        "        labels_uniq.append(uniq_group)\n",
        "        labels_uniq_loc.append(indc_group)\n",
        "    return labels_uniq, labels_uniq_loc\n",
        "\n",
        "def get_label_length_seq(content):\n",
        "    label_seq = []\n",
        "    length_seq = []\n",
        "    start = 0\n",
        "    length_seq.append(0)\n",
        "    for i in range(len(content)):\n",
        "        if content[i] != content[start]:\n",
        "            label_seq.append(content[start])\n",
        "            length_seq.append(i)\n",
        "            start = i\n",
        "    label_seq.append(content[start])\n",
        "    length_seq.append(len(content))\n",
        "\n",
        "    return label_seq, length_seq\n",
        "\n",
        "\n",
        "def get_maxpool_lstm_data(cData, indices):\n",
        "    list_data = []\n",
        "    for kkl in range(len(indices)-1):\n",
        "        cur_start = indices[kkl]\n",
        "        cur_end = indices[kkl+1]\n",
        "        if cur_end > cur_start:\n",
        "            list_data.append(torch.max(cData[cur_start:cur_end,:],\n",
        "                                       0)[0].squeeze(0))\n",
        "        else:\n",
        "            list_data.append(torch.max(cData[cur_start:cur_end+1,:],\n",
        "                                       0)[0].squeeze(0))\n",
        "    list_data  =  torch.stack(list_data)\n",
        "    return list_data\n",
        "\n",
        "def read_mapping_dict(mapping_file):\n",
        "    file_ptr = open(mapping_file, 'r')\n",
        "    actions = file_ptr.read().split('\\n')[:-1]\n",
        "\n",
        "    actions_dict=dict()\n",
        "    for a in actions:\n",
        "        actions_dict[a.split()[1]] = int(a.split()[0])\n",
        "\n",
        "    return actions_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYNJ8LtXZ7F3",
        "colab_type": "text"
      },
      "source": [
        "Due to memory constraints on colab I had to further split the files into 3 files each for a total of 3*4=12 files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8IsX-1tO3ZZ",
        "colab_type": "code",
        "outputId": "1542078e-1d76-4950-b981-b4385acfbe07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "split = 'test'\n",
        "COMP_PATH = path\n",
        "\n",
        "train_split =  os.path.join(COMP_PATH, 'splits/train.split22.bundle')\n",
        "test_split  =  os.path.join(COMP_PATH, 'splits/test.split4.bundle')\n",
        "GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/')\n",
        "DATA_folder =  os.path.join(COMP_PATH, 'data/')\n",
        "mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt')\n",
        "\n",
        "\n",
        "actions_dict = read_mapping_dict(mapping_loc)\n",
        "if split == 'train':\n",
        "    data_feat, data_labels, tasks_labels = load_data(train_split, actions_dict, GT_folder, DATA_folder)\n",
        "else:\n",
        "    data_feat, data_labels, tasks_labels= load_data( test_split, actions_dict, GT_folder, DATA_folder)\n",
        "#print(data_feat.shape,data_labels.shape,tasks_labels.shape)\n",
        "print('total number videos ' +  str(len(data_labels))  )\n",
        " "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 576/576 [10:53<00:00,  1.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total number videos 576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B4Qig7pa40e",
        "colab_type": "text"
      },
      "source": [
        "The split was made such that each file contains 500 video segments "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qVxc-2Oxz8",
        "colab_type": "code",
        "outputId": "d6c723f8-8051-48b7-8834-f0a2c2fe8098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(data_feat),len(data_labels),len(tasks_labels))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "576 576 576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LDgozGzbKcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxv=0\n",
        "for i in range(len(data_feat)):\n",
        "  if data_feat[i].shape[0]>maxv:\n",
        "    maxv=data_feat[i].shape[0]\n",
        "    \n",
        "print(maxv)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVb-cMeSbpMC",
        "colab_type": "text"
      },
      "source": [
        "Each video segment is of different length and needs to be padded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Cnx9hIqiKi",
        "colab_type": "code",
        "outputId": "8ac839f8-7e42-48cc-cab9-e4e41c1210c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_feat[0].shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([453, 400])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FcmmDaDdh3I",
        "colab_type": "text"
      },
      "source": [
        "Padding the features tensor with zero vectors and the data labels with 0 (SILENCE) and converting the labels to one hot encodings (of 48 classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOhPF0FmnDHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_feat=sequence.pad_sequences(data_feat, maxlen=8500,padding='post', truncating='post', value=0.0)\n",
        "data_labels=sequence.pad_sequences(data_labels, maxlen=8500,padding='post', truncating='post', value=0)\n",
        "data_labels = [to_categorical(i, 48) for i in data_labels]\n",
        "data_feat=np.array(data_feat)\n",
        "data_labels=np.array(data_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEG8Z7cGeHJ7",
        "colab_type": "text"
      },
      "source": [
        "The task labels contains a number between 0-9 indicating 10 different tasks i.e. cereals, coffee, fried egg, milk, salat, sandwich, tea, scrambled egg, pancake, juice\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I74XZG4xRqVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tasks_labels=[to_categorical(i,10) for i in tasks_labels]\n",
        "tasks_labels=np.array(tasks_labels)\n",
        "tasks_labels=tasks_labels.reshape(-1,1,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8ooxlipe4fI",
        "colab_type": "text"
      },
      "source": [
        "Splitting the data into train and validation to evaluate model performance\n",
        "I used a 90-10 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLaglRHDj2QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#X_train, X_val, y_train, y_val = train_test_split(data_feat, tasks_labels, test_size=0.1, random_state=420)\n",
        "X_train, X_val, y_train, y_val = train_test_split(data_feat, data_labels, test_size=0.1, random_state=420)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0VAuU8AfGDU",
        "colab_type": "text"
      },
      "source": [
        "The model is described below, It consists of two bidirectional LSTM layers of 400 output values each followed by a Max pooling layer over each LSTM cell which gives an output vector of size 800\n",
        "Finally a Dense softmax layer follows which does the actual classification into one of the 10 activities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9aAxk-SOxsq",
        "colab_type": "text"
      },
      "source": [
        "## Task classification\n",
        "### The first model is for the task classification problem \n",
        "### 1) LSTM model for task classification\n",
        "Here I use a LSTM model for reasons explained in the abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SNHwp_SFWm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(400,return_sequences=True),input_shape=(None,400)))\n",
        "model.add(Bidirectional(LSTM(400,return_sequences=True)))\n",
        "model.add(MaxPooling1D(pool_size=8500, strides=None, padding='valid', data_format='channels_last'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx0EHL4jI74H",
        "colab_type": "code",
        "outputId": "6ecaa3cf-cc1b-4650-934f-ba63501d6a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_5 (Bidirection (None, None, 800)         2563200   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, None, 800)         3843200   \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, None, 800)         0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, None, 10)          8010      \n",
            "=================================================================\n",
            "Total params: 6,414,410\n",
            "Trainable params: 6,414,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sowm5fhkH_ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model(path+\"task_classification.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFES0VBAPLpQ",
        "colab_type": "text"
      },
      "source": [
        "Using Keras callbacks to save only the best model aftereach epoch\n",
        "\n",
        "Due to memory and time constraints in colab I had to:\n",
        "\n",
        "\n",
        "*   Further split the input files\n",
        "*   Train for 3 epochs only (Training rate of Bi-LSTM is incredibly slow)\n",
        "*   Use a small batch size as colab crashed on increasing it any further\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxqUJ5tfI9lZ",
        "colab_type": "code",
        "outputId": "2d39166f-5aa6-4219-bdd1-8f59f25cb560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "best_model_file = path+\"task_classification.h5\"\n",
        "best_model = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print('Training model...')\n",
        "results = model.fit(X_train, y_train, epochs=3, batch_size=4, validation_data=(X_val, y_val), callbacks=[best_model])\n",
        "\n",
        "print('Training finished.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Train on 414 samples, validate on 46 samples\n",
            "Epoch 1/3\n",
            "340/414 [=======================>......] - ETA: 12:08 - loss: 0.6186 - acc: 0.7324"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBiYXBf6QIfV",
        "colab_type": "text"
      },
      "source": [
        "Model was evaluated later after importing the test bundle and it has an accuracy of 78.47%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLr3W1r3Yu6V",
        "colab_type": "code",
        "outputId": "5b009cb9-01f4-4843-c66c-f301b551ed20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(data_feat, tasks_labels) # This is actually the test data I imported later and not the train data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "576/576 [==============================] - 190s 330ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5722099526060952, 0.7847222222222222]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cuOx3U_QYnD",
        "colab_type": "text"
      },
      "source": [
        "## Activity classification\n",
        "### 2) Simple neural network model for activity classification\n",
        "Now moving on to the primary task of activity classification\n",
        "\n",
        "First I try a simple neural network for classifying the feature vectors into one of the 48 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvWUy6UDxSKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(256,activation='relu',input_shape=(None,400)))\n",
        "model2.add(Dense(128,activation='relu'))\n",
        "model2.add(Dense(128,activation='relu'))\n",
        "\n",
        "\n",
        "model2.add(Dense(48, activation='softmax'))\n",
        "model2.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uclTqknFNg1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2=load_model(path+\"label_classification.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmq8edEjKpSE",
        "colab_type": "code",
        "outputId": "8ebab6ff-b2b8-4300-9947-6b1c923a76c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "best_model2_file = path+\"label_classification.h5\"\n",
        "best_model2 = ModelCheckpoint(best_model2_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "print('Training model...')\n",
        "results = model2.fit(X_train, y_train, epochs=20, batch_size=4, validation_data=(X_val, y_val), callbacks=[best_model2])\n",
        "\n",
        "print('Training finished.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Train on 449 samples, validate on 50 samples\n",
            "Epoch 1/20\n",
            "449/449 [==============================] - 8s 18ms/step - loss: 0.5189 - acc: 0.8500 - val_loss: 0.5757 - val_acc: 0.8391\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.57570, saving model to /content/gdrive/My Drive/video_action_classification/nndataexlabel_classification.h5\n",
            "Epoch 2/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.4691 - acc: 0.8600 - val_loss: 0.5333 - val_acc: 0.8427\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.57570 to 0.53334, saving model to /content/gdrive/My Drive/video_action_classification/nndataexlabel_classification.h5\n",
            "Epoch 3/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.4464 - acc: 0.8659 - val_loss: 0.5288 - val_acc: 0.8460\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.53334 to 0.52883, saving model to /content/gdrive/My Drive/video_action_classification/nndataexlabel_classification.h5\n",
            "Epoch 4/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.4322 - acc: 0.8693 - val_loss: 0.5325 - val_acc: 0.8419\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.52883\n",
            "Epoch 5/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.4118 - acc: 0.8744 - val_loss: 0.5280 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.52883 to 0.52803, saving model to /content/gdrive/My Drive/video_action_classification/nndataexlabel_classification.h5\n",
            "Epoch 6/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3943 - acc: 0.8790 - val_loss: 0.5308 - val_acc: 0.8494\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.52803\n",
            "Epoch 7/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3825 - acc: 0.8827 - val_loss: 0.5095 - val_acc: 0.8554\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.52803 to 0.50953, saving model to /content/gdrive/My Drive/video_action_classification/nndataexlabel_classification.h5\n",
            "Epoch 8/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3690 - acc: 0.8863 - val_loss: 0.5222 - val_acc: 0.8491\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.50953\n",
            "Epoch 9/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3566 - acc: 0.8909 - val_loss: 0.5280 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.50953\n",
            "Epoch 10/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3456 - acc: 0.8927 - val_loss: 0.5198 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.50953\n",
            "Epoch 11/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3394 - acc: 0.8944 - val_loss: 0.5421 - val_acc: 0.8457\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.50953\n",
            "Epoch 12/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3437 - acc: 0.8929 - val_loss: 0.5605 - val_acc: 0.8481\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.50953\n",
            "Epoch 13/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3241 - acc: 0.8988 - val_loss: 0.5344 - val_acc: 0.8498\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.50953\n",
            "Epoch 14/20\n",
            "449/449 [==============================] - 7s 17ms/step - loss: 0.3209 - acc: 0.8996 - val_loss: 0.5423 - val_acc: 0.8485\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.50953\n",
            "Epoch 15/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3118 - acc: 0.9031 - val_loss: 0.5422 - val_acc: 0.8476\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.50953\n",
            "Epoch 16/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.3002 - acc: 0.9060 - val_loss: 0.5315 - val_acc: 0.8528\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.50953\n",
            "Epoch 17/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.2957 - acc: 0.9078 - val_loss: 0.5488 - val_acc: 0.8496\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.50953\n",
            "Epoch 18/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.2867 - acc: 0.9098 - val_loss: 0.5598 - val_acc: 0.8504\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.50953\n",
            "Epoch 19/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.2900 - acc: 0.9081 - val_loss: 0.5714 - val_acc: 0.8474\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.50953\n",
            "Epoch 20/20\n",
            "449/449 [==============================] - 7s 16ms/step - loss: 0.2893 - acc: 0.9081 - val_loss: 0.5663 - val_acc: 0.8484\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.50953\n",
            "Training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKFgkf7KQr_F",
        "colab_type": "text"
      },
      "source": [
        "The training process took a few seconds for each training bundle and gave a test accuracy of 87.4%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv28Tld2Aqb-",
        "colab_type": "code",
        "outputId": "6a321dfe-bf51-49ee-99ca-672a243cc06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model2.evaluate(data_feat, data_labels) # This is actually the test data I imported later and not the train data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "576/576 [==============================] - 8s 13ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.43752316137154895, 0.874196492963367]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1VWs7onQ2LU",
        "colab_type": "text"
      },
      "source": [
        "### 3) LSTM model for activity classification\n",
        "There is still scope for improvement, let's see if an LSTM model is able to outperform the vanilla neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8zaV-PbBCBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "de2bff2b-d7c0-45ae-e4a4-b2e5f5430765"
      },
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Bidirectional(LSTM(400,return_sequences=True),input_shape=(None,400)))\n",
        "\n",
        "\n",
        "model3.add(Dense(48, activation='softmax'))\n",
        "model3.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_2 (Bidirection (None, None, 800)         2563200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 48)          38448     \n",
            "=================================================================\n",
            "Total params: 2,601,648\n",
            "Trainable params: 2,601,648\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIQbTtKFLLE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2489d12f-98aa-42f1-dbf1-5d5abc6637f0"
      },
      "source": [
        "results = model3.fit(X_train, y_train, epochs=20, batch_size=4, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 518 samples, validate on 58 samples\n",
            "Epoch 1/20\n",
            "392/518 [=====================>........] - ETA: 14:02 - loss: 0.8984 - acc: 0.8066"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSNhy23QLSxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}